{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f036306-416d-4ef8-86fd-8a965155892d",
   "metadata": {},
   "source": [
    "# Comparing Performance of Neural Network Architectures Using Data on Wildfire Propagation\n",
    "\n",
    "## Author: Alec Creasy\n",
    "\n",
    "### This notebook/script allows you to re-create my experiment in my Final paper for CSCI 6620 - Research Methods in Computer Science at Middle Tennessee State University.\n",
    "\n",
    "## IMPORTANT: Before running this notebook, make sure you have installed all dependencies and be sure you get a Map API key for the satellite data! (Info in the README.md file.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7a012-427e-4902-bf4c-0b74bffd0c5e",
   "metadata": {},
   "source": [
    "# Collection of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79132ade-1c39-4579-acb1-668700712bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "from torchview import draw_graph\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import torchmetrics\n",
    "\n",
    "# Define your MAP_KEY\n",
    "MAP_KEY = 00000000000000000  # Replace with your actual MAP_KEY\n",
    "\n",
    "# Try and get the data. If valid, you will see information pertaining to your key. Otherwise\n",
    "# an error will be reported. This usually has to do with having an invalid API key.\n",
    "url = 'https://firms.modaps.eosdis.nasa.gov/mapserver/mapkey_status/?MAP_KEY=' + MAP_KEY\n",
    "try:\n",
    "  df = pd.read_json(url,  typ='series')\n",
    "  display(df)\n",
    "except:\n",
    "  # possible error, wrong MAP_KEY value, check for extra quotes, missing letters\n",
    "  print (\"There is an issue with the query. \\nTry in your browser: %s\" % url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038d7df-78ad-447f-8831-67e8e0ce8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define time range\n",
    "start_date = '2018-11-01'\n",
    "end_date   = '2018-12-01'\n",
    "\n",
    "#Build FIRMS API URL. The URL contains the dates to look for data (November 1, 2018-December 1, 2018) and the latitude and longitude range\n",
    "# (Butte County, CA)\n",
    "modis_url = 'https://firms.modaps.eosdis.nasa.gov/api/area/csv/' + MAP_KEY + f'/MODIS_SP/-122,39.5,-121.2,40.1/10/2018-11-01/2018-12-01'\n",
    "\n",
    "# Get MODIS Satellite data from November 2, 2018-November 10, 2018, when the Butte County fires took place.\n",
    "df_area = pd.read_csv(modis_url)\n",
    "\n",
    "#Displays the dataset\n",
    "display(df_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09da811-40eb-4afa-9a03-baf4a4849fc8",
   "metadata": {},
   "source": [
    "# Pre-process the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7a661-d6c0-4f93-9538-c25a69b1ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates binary grids from the satellite data that represent whether or not an area ir burning or not.\n",
    "def build_fire_grid(df, lat_min, lat_max, lon_min, lon_max, grid_size=0.01):\n",
    "\n",
    "    # Create the latitude and longitude cells with a cell size of .01 degrees (latitude and longitude).\n",
    "    # Store the size of these as height and width of the grids, and convert the acquired\n",
    "    # dates from the data to datetime objects to extract the minimum dates and maximum dates.\n",
    "    lat_bins = np.arange(lat_min, lat_max + grid_size, grid_size)\n",
    "    lon_bins = np.arange(lon_min, lon_max + grid_size, grid_size)\n",
    "    H, W = len(lat_bins), len(lon_bins)\n",
    "    df['acq_date'] = pd.to_datetime(df['acq_date'])\n",
    "    all_dates = pd.date_range(df['acq_date'].min(), df['acq_date'].max())\n",
    "\n",
    "    # Create an empty list for the grids\n",
    "    daily_grids = []\n",
    "\n",
    "    # For each date, create an area of zeros. Then, for each cell, determine if there was a fire\n",
    "    # (that is, the latitude/longitude coordinate is in the dataset). If so, update the cell to 1 and add\n",
    "    # to the grid list.\n",
    "    for current_date in all_dates:\n",
    "        day_data = df[df['acq_date'] == current_date]\n",
    "        grid = np.zeros((H, W), dtype=np.uint8)\n",
    "        \n",
    "        lat_idx = np.digitize(day_data['latitude'], lat_bins) - 1\n",
    "        lon_idx = np.digitize(day_data['longitude'], lon_bins) - 1\n",
    "        \n",
    "        for y, x in zip(lat_idx, lon_idx):\n",
    "            if 0 <= y < H and 0 <= x < W:\n",
    "                grid[y, x] = 1\n",
    "        daily_grids.append(grid)\n",
    "\n",
    "    # Return the grid along with its height and width.\n",
    "    return np.array(daily_grids), H, W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c81018-e86c-43ab-96be-3e2fc46e8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take our daily grids and make 3x3 spatial grids with 5 time steps\n",
    "# that will be used as our data for the model.\n",
    "def generate_patch_sequences(daily_grids, seq_len=5, patch_size=3):\n",
    "    T, H, W = daily_grids.shape\n",
    "\n",
    "    # Adds spatial padding\n",
    "    pad = patch_size // 2\n",
    "    padded_grids = np.pad(daily_grids, ((0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
    "\n",
    "    # Create empty lists for our X and Y values in training (X is the\n",
    "    # 5 previous time steps, y is the 6th correct time step)\n",
    "    x_seqs, y_targets = [], []\n",
    "\n",
    "    # Loops through every pixel in the daily fire grids.\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "\n",
    "            \n",
    "            pixel_series = daily_grids[:, i, j] \n",
    "\n",
    "            # If no fire is present in this cell, skip this iteration and continue.\n",
    "            if pixel_series.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            for t in range(T - seq_len):\n",
    "                # Sequence of patches around (i, j). The x_seq is the sequence of the last 5 time steps.\n",
    "                # y is the next time step to be predicted.\n",
    "                x_seq = padded_grids[t:t+seq_len, i:i+patch_size, j:j+patch_size]  # (seq_len, patch, patch)\n",
    "                y = daily_grids[t+seq_len, i, j]\n",
    "\n",
    "                # Reshape to (seq_len, 1, patch_size, patch_size)\n",
    "                x_seqs.append(x_seq[:, None, :, :])\n",
    "                y_targets.append([[y]])\n",
    "\n",
    "    # Convert the 3x3 patches to tensors and return them\n",
    "    x_tensor = torch.tensor(x_seqs, dtype=torch.float32)  # (N, seq_len, 1, patch, patch)\n",
    "    y_tensor = torch.tensor(y_targets, dtype=torch.float32)  # (N, 1, 1)\n",
    "    return x_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e74173-3722-4839-8401-934876446a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Data Module using PyTorch Lightning. Sets up the data loaders for the training and validation\n",
    "# data, as well as splits the data into 80% training data and 20% validation data.\n",
    "class FireDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, x, y, batch_size=32, num_workers=2):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        N = len(self.x)\n",
    "        split = int(N * 0.8)\n",
    "        self.train_dataset = FireDataset(self.x[:split], self.y[:split])\n",
    "        self.val_dataset = FireDataset(self.x[split:], self.y[split:])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384d4ee7-0083-423e-a0e5-197f45478590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bounded region for Butte County, CA\n",
    "lat_min, lat_max = 39.5, 40.1\n",
    "lon_min, lon_max = -122.0, -121.2\n",
    "\n",
    "# 1. Grid the fire data\n",
    "daily_grids, H, W = build_fire_grid(df_area, lat_min, lat_max, lon_min, lon_max)\n",
    "\n",
    "# 2. Generate patch sequences\n",
    "x_tensor, y_tensor = generate_patch_sequences(daily_grids, seq_len=5, patch_size=3)\n",
    "\n",
    "# 3. Create DataModule and set it up.\n",
    "data_module = FireDataModule(x_tensor, y_tensor, batch_size=32)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a26fc-6e83-4475-9318-1c8e780e50f6",
   "metadata": {},
   "source": [
    "# Create the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80708034-17a0-4cde-9b6f-a3a406a5ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the ConvLSTM cell to be used in the ConvLSTM model. Modifies the LSTM cell to include Convolution.\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=input_channels + hidden_channels,\n",
    "                              out_channels=4 * hidden_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              padding=padding)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        conv_output = self.conv(combined)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.chunk(conv_output, 4, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_prev + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f753894-77f0-497e-8dc2-d9d303575d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a module to stack various ConvLSTM cells to be used in the ConvLSTM model.\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, kernel_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(num_layers):\n",
    "            cur_input_dim = input_dim if i == 0 else hidden_dims[i - 1]\n",
    "            cell_list.append(ConvLSTMCell(cur_input_dim, hidden_dims[i], kernel_size))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        B, T, _, H, W = input_tensor.size()\n",
    "\n",
    "        h = [torch.zeros(B, hidden_dim, H, W, device=input_tensor.device) for hidden_dim in self.hidden_dims]\n",
    "        c = [torch.zeros(B, hidden_dim, H, W, device=input_tensor.device) for hidden_dim in self.hidden_dims]\n",
    "\n",
    "        layer_outputs = []\n",
    "\n",
    "        for t in range(T):\n",
    "            x = input_tensor[:, t]  # (B, C, H, W)\n",
    "\n",
    "            for i, cell in enumerate(self.cell_list):\n",
    "                h[i], c[i] = cell(x, (h[i], c[i]))\n",
    "                x = h[i]  # pass to next layer\n",
    "\n",
    "            layer_outputs.append(h[-1])  # keep only last layer's output\n",
    "\n",
    "        output_seq = torch.stack(layer_outputs, dim=1)\n",
    "        return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72ccd4-aaba-46a5-8e48-e496c5e6319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the ConvLSTM Model.\n",
    "class ConvLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, input_channels=1, hidden_dims=[64, 128, 256], kernel_size=3, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Creates the stacked ConvLSTM layers.\n",
    "        self.convlstm = ConvLSTM(\n",
    "            input_dim=input_channels,\n",
    "            hidden_dims=hidden_dims,\n",
    "            kernel_size=kernel_size,\n",
    "            num_layers=len(hidden_dims)\n",
    "        )\n",
    "\n",
    "        # Final 3D conv layer to collapse time + channel dims\n",
    "        self.final_conv = nn.Conv3d(\n",
    "            in_channels=hidden_dims[-1],\n",
    "            out_channels=1,\n",
    "            kernel_size=(3, 3, 3),\n",
    "            padding=(1, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.train_f1 = torchmetrics.classification.BinaryF1Score(threshold=0.5)\n",
    "        self.val_f1 = torchmetrics.classification.BinaryF1Score(threshold=0.5)\n",
    "        self.test_f1 = torchmetrics.classification.BinaryF1Score(threshold=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.convlstm(x) \n",
    "        features = features.permute(0, 2, 1, 3, 4) \n",
    "\n",
    "        out = self.final_conv(features)           \n",
    "        out = self.sigmoid(out[:, :, -1])         \n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        center = y_hat[:, :, 1, 1]\n",
    "        loss = F.binary_cross_entropy(center, y.squeeze(-1))\n",
    "        acc = self.train_f1(center, y.squeeze(-1))\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        center = y_hat[:, :, 1, 1]\n",
    "        loss = F.binary_cross_entropy(center, y.squeeze(-1))\n",
    "        acc = self.val_f1(center, y.squeeze(-1))\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        center = y_hat[:, :, 1, 1]\n",
    "        loss = F.binary_cross_entropy(center, y.squeeze(-1))\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24ec27-0cf6-4c5a-ab79-ed482c78979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the ConvGRU cell to be used in the ConvGRU model. Modifies the GRU cell to include Convolution.\n",
    "class ConvGRUCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.reset_gate = nn.Conv2d(input_dim + hidden_dim, hidden_dim, kernel_size, padding=padding, bias=bias)\n",
    "        self.update_gate = nn.Conv2d(input_dim + hidden_dim, hidden_dim, kernel_size, padding=padding, bias=bias)\n",
    "        self.out_gate = nn.Conv2d(input_dim + hidden_dim, hidden_dim, kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(x.size(0), self.hidden_dim, x.size(2), x.size(3), device=x.device)\n",
    "\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        reset = torch.sigmoid(self.reset_gate(combined))\n",
    "        update = torch.sigmoid(self.update_gate(combined))\n",
    "\n",
    "        combined_reset = torch.cat([x, reset * h_prev], dim=1)\n",
    "        out = torch.tanh(self.out_gate(combined_reset))\n",
    "\n",
    "        h_new = (1 - update) * h_prev + update * out\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea3d3e-d076-470c-84ec-de8e511faeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a module to stack various ConvGRU cells to be used in the ConvGRU model.\n",
    "class ConvGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, kernel_size, num_layers):\n",
    "        super().__init__()\n",
    "        assert len(hidden_dims) == num_layers, \"hidden_dims must match num_layers\"\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        cells = []\n",
    "        for i in range(num_layers):\n",
    "            cur_input_dim = input_dim if i == 0 else hidden_dims[i - 1]\n",
    "            cell = ConvGRUCell(cur_input_dim, hidden_dims[i], kernel_size)\n",
    "            cells.append(cell)\n",
    "\n",
    "        self.cells = nn.ModuleList(cells)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _, H, W = x.size()\n",
    "        h = [None] * self.num_layers\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(T):\n",
    "            input_t = x[:, t]\n",
    "            for i, cell in enumerate(self.cells):\n",
    "                h[i] = cell(input_t, h[i])\n",
    "                input_t = h[i] \n",
    "            outputs.append(h[-1].unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9a19d-7133-438d-b981-1660a03c9031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the ConvGRU model using the ConvGRU stacked layers.\n",
    "class ConvGRUModel(pl.LightningModule):\n",
    "    def __init__(self, input_channels=1, hidden_dims=[64, 128, 256], kernel_size=3, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Creates the stacked ConvGRU layers.\n",
    "        self.convgru = ConvGRU(\n",
    "            input_dim=input_channels,\n",
    "            hidden_dims=hidden_dims,\n",
    "            kernel_size=kernel_size,\n",
    "            num_layers=len(hidden_dims)\n",
    "        )\n",
    "\n",
    "        # Final 3D conv layer to collapse time + channel dims\n",
    "        self.final_conv = nn.Conv3d(\n",
    "            in_channels=hidden_dims[-1],\n",
    "            out_channels=1,\n",
    "            kernel_size=(3, 3, 3),\n",
    "            padding=(1, 1, 1)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.train_f1 = torchmetrics.classification.BinaryF1Score(threshold=0.5)\n",
    "        self.val_f1 = torchmetrics.classification.BinaryF1Score(threshold=0.5)\n",
    "        self.test_f1 = torchmetrics.classification.BinaryF1Score(threshold=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.convgru(x)\n",
    "        features = features.permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        out = self.final_conv(features)\n",
    "        out = self.sigmoid(out[:, :, -1])\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        center = y_hat[:, :, 1, 1]\n",
    "        loss = F.binary_cross_entropy(center, y.squeeze(-1))\n",
    "        acc = self.train_f1(center, y.squeeze(-1))\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('train_acc', acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        center = y_hat[:, :, 1, 1]\n",
    "        loss = F.binary_cross_entropy(center, y.squeeze(-1))\n",
    "        acc = self.val_f1(center, y.squeeze(-1))\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        center = y_hat[:, :, 1, 1]\n",
    "        loss = F.binary_cross_entropy(center, y.squeeze(-1))\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1bf36d-4211-4cc9-b8fc-07c98edce0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ConvLSTM model and print a summary of the model\n",
    "lstm_model = ConvLSTMModel()\n",
    "summary(lstm_model, input_size=(32, 5, 1, 32, 32))  # (B, T, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539791cb-68e4-4a5c-90c5-7b30e7c53e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ConvGRU model and print a summary of the model\n",
    "gru_model = ConvGRUModel()\n",
    "summary(gru_model, input_size=(32, 5, 1, 32, 32))  # (B, T, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851c492-7ec2-46d4-b25e-9bf8c4c581b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model the ConvLSTM model.\n",
    "model_graph = draw_graph(lstm_model, input_size=(32, 5, 1, 3, 3), depth=1)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da2627-4244-4238-a7fa-52ae0e22e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model the ConvGRU model.\n",
    "model_graph = draw_graph(gru_model, input_size=(32, 5, 1, 3, 3), depth=1)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f7d5f1-e5b4-4fc9-82ad-b2571730f682",
   "metadata": {},
   "source": [
    "# Training the ConvLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dcb0a-10a5-4e6e-a158-1a91278da11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger(\"logs\",\n",
    "                              name=\"LSTM-Log\",\n",
    "                             version=\"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017fa03e-5b04-43aa-a310-187a78b03fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(logger=logger,\n",
    "                     max_epochs=50,\n",
    "                     enable_progress_bar=True,\n",
    "                     log_every_n_steps=0,\n",
    "                     callbacks=[pl.callbacks.TQDMProgressBar(refresh_rate=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8326fb-ec47-446c-a148-745dd76ff9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(lstm_model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c29b6-ebc0-4e9b-9a5b-b97b5605ea62",
   "metadata": {},
   "source": [
    "# Training the ConvGRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46192304-dc79-4211-8e99-5cc75f3285ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger(\"logs\",\n",
    "                              name=\"GRU-Log\",\n",
    "                             version=\"gru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747826e-7056-491b-96c3-5bb61a44c99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(logger=logger,\n",
    "                     max_epochs=50,\n",
    "                     enable_progress_bar=True,\n",
    "                     log_every_n_steps=0,\n",
    "                     callbacks=[pl.callbacks.TQDMProgressBar(refresh_rate=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccb63e-76f5-4a91-aa30-d114bfddb88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(gru_model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6ce07-4e77-4877-a948-df7996fd59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_results = pd.read_csv(\"logs/LSTM-Log/lstm/metrics.csv\")\n",
    "gru_results = pd.read_csv(\"logs/GRU-Log/gru/metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f253824c-17af-4f45-95ae-3deaceaca278",
   "metadata": {},
   "source": [
    "# Plotting of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fbb6a-d77b-4938-b5f8-048c95dfe7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_results[\"epoch\"][np.logical_not(np.isnan(lstm_results[\"train_loss\"]))],\n",
    "         lstm_results[\"train_loss\"][np.logical_not(np.isnan(lstm_results[\"train_loss\"]))],\n",
    "         label=\"LSTM\")\n",
    "plt.plot(gru_results[\"epoch\"][np.logical_not(np.isnan(gru_results[\"train_loss\"]))],\n",
    "         gru_results[\"train_loss\"][np.logical_not(np.isnan(gru_results[\"train_loss\"]))],\n",
    "         label=\"GRU\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d63d7-7714-4fc8-ac3f-c185af5a5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_results[\"epoch\"][np.logical_not(np.isnan(lstm_results[\"train_acc\"]))],\n",
    "         lstm_results[\"train_acc\"][np.logical_not(np.isnan(lstm_results[\"train_acc\"]))],\n",
    "         label=\"LSTM\")\n",
    "plt.plot(gru_results[\"epoch\"][np.logical_not(np.isnan(gru_results[\"train_acc\"]))],\n",
    "         gru_results[\"train_acc\"][np.logical_not(np.isnan(gru_results[\"train_acc\"]))],\n",
    "         label=\"GRU\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6511d-14bb-469f-9a16-09e54dd167a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_results[\"epoch\"][np.logical_not(np.isnan(lstm_results[\"val_loss\"]))],\n",
    "         lstm_results[\"val_loss\"][np.logical_not(np.isnan(lstm_results[\"val_loss\"]))],\n",
    "         label=\"ConvLSTM\")\n",
    "plt.plot(gru_results[\"epoch\"][np.logical_not(np.isnan(gru_results[\"val_loss\"]))],\n",
    "         gru_results[\"val_loss\"][np.logical_not(np.isnan(gru_results[\"val_loss\"]))],\n",
    "         label=\"ConvGRU\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"BCE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b9027-33f6-48d3-924c-0ffbbeca095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_results[\"epoch\"][np.logical_not(np.isnan(lstm_results[\"val_acc\"]))],\n",
    "         lstm_results[\"val_acc\"][np.logical_not(np.isnan(lstm_results[\"val_acc\"]))],\n",
    "         label=\"ConvLSTM\")\n",
    "plt.plot(gru_results[\"epoch\"][np.logical_not(np.isnan(gru_results[\"val_acc\"]))],\n",
    "         gru_results[\"val_acc\"][np.logical_not(np.isnan(gru_results[\"val_acc\"]))],\n",
    "         label=\"ConvGRU\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Validation F1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07f25f-91ce-4118-acd8-6c8ce585b3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graph = draw_graph(model, input_size=(32, 5, 1, 3, 3), depth=1)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305f619-c24f-4e62-bae4-8dbd5e206781",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
